{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from auxiliary import *\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression estimators of causal effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Overivew**\n",
    "\n",
    "* Regression as a descriptive tool\n",
    "\n",
    "* Regression adjustment as a strategy to to estimate causal effects\n",
    "    * Regression models and omitted-variable bias\n",
    "    * Potential outcomes and omitted-variable bias\n",
    "    * Regression as adjustment for otherwise omitted variables\n",
    "\n",
    "* Regression as conditional-variance-weighted matching\n",
    "\n",
    "* Regression as an implementation of a perfect stratification\n",
    "\n",
    "* Regression as supplemental adjustment when matching\n",
    "\n",
    "* Extensions and other perspectives\n",
    "    * Regression estimators for many-valued causes\n",
    "    * The challenge of regression specification\n",
    "\n",
    "* Conclusion\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We start with different ways of using regression\n",
    "\n",
    "* descriptive tools\n",
    "    * Anscombe quartet\n",
    "    \n",
    "* estimating causal effects\n",
    "\n",
    "* Freedman's paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression as a descriptive tool\n",
    "\n",
    "Goldberger (1991) motivates least squares regression as a technique to estimate a best-fitting linear approximation to a conditional expectation function that may be nonlinear in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/regression_demonstration_one.png\" height=300 width=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the functional form of the conditiional expectation look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What does the difference between the two lines tell us about treatment effect heterogeneity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will fit four different prediction models using ordinary least squares.\n",
    "\n",
    "\\begin{align*}\n",
    "&\\hat{Y} = \\beta_0 + \\beta_1 D + \\beta_2 S \\\\\n",
    "&\\hat{Y} = \\beta_0 + \\beta_1 D + \\beta_2 S \\\\\n",
    "&\\hat{Y} = \\beta_0 + \\beta_1 D + \\beta_2 S_1 + \\beta_3 S_2 \\\\\n",
    "&\\hat{Y} = \\beta_0 + \\beta_1 D + \\beta_2 S_1 + \\beta_3 S_2 + \\beta_4 S_1 * D + \\beta_5 S_2 * D  \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anscombe quartet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So what does the data behind these regressions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression adjustment as a strategy to estimate causal effects\n",
    "\n",
    "### Regression models and omitted-variable bias\n",
    "\n",
    "\\begin{align*}\n",
    "Y = \\alpha + \\delta D + \\epsilon\n",
    "\\end{align*}\n",
    "\n",
    "* $\\delta$ is interpreted as an invariant, structural causal effect that applies to all members of the population.\n",
    "\n",
    "* $\\epsilon$ is a summary random variable that represents all other causes of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\hat{\\delta}_{OLS, \\text{bivariate}} = \\frac{Cov_N(y_i, d_i)}{Var_N(d_i)}\n",
    "\\end{align*}\n",
    "\n",
    "It now depends on the correlation between $\\epsilon$ and $D$ whether $\\hat{\\delta}$ provides an unbiased and consistent estimate of the true causal effect\n",
    "\n",
    "<img src=\"material/omitted-variable-bias.png\" height=300 width=300 />\n",
    "\n",
    "We now move to the potential outcomes model to clarify the connection between **omitted-variable bias** and **self-selection bias**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Potential outcomes and omitted-variable bias\n",
    "\n",
    "\\begin{align*}\n",
    "Y = \\underbrace{\\mu^0}_{\\alpha} + \\underbrace{(\\mu^1 - \\mu^0)}_{\\delta} D + \\underbrace{\\{\\nu^0 + D(\\nu^1 - \\nu^0 )\\}}_{\\epsilon}, \n",
    "\\end{align*}\n",
    "\n",
    "where $\\mu^0\\equiv E[Y^0]$, $\\mu^1\\equiv E[Y^1]$, $\\nu^0\\equiv Y^0 - E[Y^0]$, and $\\nu^1\\equiv Y^1 - E[Y^1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What induces a correlation between $D$ an $\\{\\nu^0 + D(\\nu^1 - \\nu^0 )\\}$?\n",
    "\n",
    "* **baseline bias**, there is a net baseline difference in the hypotheticsl no-treatment state that is correlated with treatment uptake\n",
    "$\\rightarrow$ $D$ is correlated with $\\nu_0$\n",
    "\n",
    "* **differential treatment bias**, there is a net treatment effect difference that is correlated with treatment updatake\n",
    "$\\rightarrow$ $D$ is correlated with $D(\\nu^1 - \\nu^0 )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/regression_demonstration_two.png\" height=300 width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression as adjustment for otherwise omitted variables\n",
    "\n",
    "<img src=\"material/observable-regression-adjustment.png\" height=300 width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/regression_demonstration_three.png\" height=300 width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now look at two datasets that are observationally equivalent but regression adjustment for observable $X$ does only work in one of them.\n",
    "\n",
    "<img src=\"material/regression_demonstration_four.png\" height=300 width=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we condition on $X$ to see where conditioning might help in obtaining an unbiased estimate of the true effect.\n",
    "\n",
    "<img src=\"material/regression_demonstration_five.png\" height=300 width=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To summarize: Regression adjustment by $X$ will yield a consistent and unbiased estimate of the ATE when:\n",
    "\n",
    "* $D$ is mean independent of (and therefore uncorrelated with) $v^0 + D(v^1 - v^0)$ for each subset of respondent identified by distinct values on teh variables in $X$\n",
    "\n",
    "* the causal effect of $D$ does not vary with $X$ \n",
    "\n",
    "* a fully flexible parameterization of $X$ is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Freedman's paradox\n",
    "\n",
    "> In statistical analysis, Freedman's paradox, named after David Freedman, is a problem in model selection whereby predictor variables with no relationship to the dependent variable can pass tests of significance â€“ both individually via a t-test, and jointly via an F-test for the significance of the regression. (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We fill a dataframe with random numbers. Thus there is no causal relationshop between the dependent and independent  variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we run a simple regression of the random independent variables on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We use this to inform a second regression where we only keep the variables that were significant at the 25% level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What to make of this exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
